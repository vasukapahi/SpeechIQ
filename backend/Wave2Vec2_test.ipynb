{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52466448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_audio(filename='user_input.wav', duration=3, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    write(filename, fs, audio)\n",
    "    print(f\"Recording finished and saved as {filename}\")\n",
    "    return filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11241ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "def extract_features_from_audio(audio_path, processor, wav2vec_model, device):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        features = wav2vec_model(inputs.input_values.to(device)).last_hidden_state.squeeze(0)\n",
    "    return features.cpu(), features.shape[0]  # features, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0138f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_csv = 'train_data.csv'\n",
    "\n",
    "\n",
    "def add_intent_column(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['intent'] = df['action'].fillna('') + '_' + df['object'].fillna('') + '_' + df['location'].fillna('')\n",
    "    df['intent'] = df['intent'].str.strip('_')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df\n",
    "\n",
    "train_df = add_intent_column(train_csv)\n",
    "\n",
    "# For training data\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df['intent'] = train_df['action'].fillna('') + \"_\" + \\\n",
    "                     train_df['object'].fillna('') + \"_\" + \\\n",
    "                     train_df['location'].fillna('')\n",
    "\n",
    "\n",
    "all_labels = train_df['intent'].unique().tolist()\n",
    "label2idx = {lbl: i for i, lbl in enumerate(all_labels)}\n",
    "idx2label = {i: lbl for lbl, i in label2idx.items()}\n",
    "num_classes = len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c2b2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331ea5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN_GRU_Model(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "        self.gru = nn.GRU(128, 128, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(128*2, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [batch, time, feat]\n",
    "        x = x.transpose(1,2) # [batch, feat, time]\n",
    "        x = self.cnn(x)\n",
    "        x = x.transpose(1,2) # [batch, time, feat]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        last_outputs = out[torch.arange(out.size(0)), lengths-1]\n",
    "        logits = self.classifier(last_outputs)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244a0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68885429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_GRU_Model(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv1d(768, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gru): GRU(128, 128, batch_first=True, bidirectional=True)\n",
       "  (classifier): Linear(in_features=256, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruct your model architecture\n",
    "input_dim = 768  # Wav2Vec2 output dim (check your model)\n",
    "num_classes = len(label2idx)  # You already have this\n",
    "\n",
    "model = CNN_GRU_Model(input_dim, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('best_cnn_gru_model_2.pt', map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1b5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(audio_path, processor, wav2vec_model, model, device, idx2label):\n",
    "    features, length = extract_features_from_audio(audio_path, processor, wav2vec_model, device)\n",
    "    # Prepare batch dimensions\n",
    "    features = features.unsqueeze(0).to(device)  # [1, time, feat]\n",
    "    lengths = torch.tensor([length]).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(features, lengths)\n",
    "        pred_idx = logits.argmax(dim=1).item()\n",
    "        intent = idx2label[pred_idx]\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef9deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavgogia/Desktop/Speech Intent Recoginition/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2SdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "wav2vec_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ebdb5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished and saved as user_input.wav\n",
      "Predicted intent: decrease_volume_none\n"
     ]
    }
   ],
   "source": [
    "# 1. Record audio\n",
    "audio_path = record_audio(duration=3) \n",
    "\n",
    "# 2. Predict intent\n",
    "intent = predict_intent(audio_path, processor, wav2vec_model, model, device, idx2label)\n",
    "print(\"Predicted intent:\", intent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a773951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e3dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
